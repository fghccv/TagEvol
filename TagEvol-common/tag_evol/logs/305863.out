instruct generation
tag_evol_tag3_7b_yxprompt
INFO 03-29 14:34:25 __init__.py:207] Automatically detected platform cuda.
WARNING 03-29 14:34:25 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-29 14:34:33 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 03-29 14:34:33 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 14:34:33 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 14:34:34 multiproc_worker_utils.py:300] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 14:34:34 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:34 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:35 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:35 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:35 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:35 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:35 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:35 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:36 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:36 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:36 cuda.py:226] Using XFormers backend.
INFO 03-29 14:34:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-29 14:34:36 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:36 cuda.py:226] Using XFormers backend.
INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:38 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2893844)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893845)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893848)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m WARNING 03-29 14:34:39 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 03-29 14:34:39 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_30999edd'), local_subscribe_port=59635, remote_subscribe_port=None)
INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:34:39 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:38:51 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:38:51 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:38:51 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:38:52 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:38:52 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:38:52 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:38:52 model_runner.py:1115] Loading model weights took 16.9989 GB
INFO 03-29 14:38:52 model_runner.py:1115] Loading model weights took 16.9989 GB
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.23 seconds
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.19GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.19GiB.
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.22 seconds
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.14GiB.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.25 seconds
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.14GiB.
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.32 seconds
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.14GiB.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.25 seconds
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.19GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.19GiB.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.25 seconds
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.14GiB.
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.30 seconds
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.19GiB.
INFO 03-29 14:40:47 worker.py:267] Memory profiling takes 115.38 seconds
INFO 03-29 14:40:47 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
INFO 03-29 14:40:47 worker.py:267] model weights take 17.00GiB; non_torch_memory takes 0.25GiB; PyTorch activation peak memory takes 3.19GiB; the rest of the memory reserved for KV Cache is 8.13GiB.
INFO 03-29 14:40:48 executor_base.py:111] # cuda blocks: 13318, # CPU blocks: 6553
INFO 03-29 14:40:48 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 6.50x
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:41:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:41:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:41:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:41:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:41:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-29 14:41:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:41:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:41:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m INFO 03-29 14:42:03 model_runner.py:1562] Graph capturing finished in 61 secs, took 3.03 GiB
[1;36m(VllmWorkerProcess pid=2893845)[0;0m INFO 03-29 14:42:03 model_runner.py:1562] Graph capturing finished in 61 secs, took 3.04 GiB
[1;36m(VllmWorkerProcess pid=2893848)[0;0m INFO 03-29 14:42:03 model_runner.py:1562] Graph capturing finished in 63 secs, took 3.03 GiB
[1;36m(VllmWorkerProcess pid=2893844)[0;0m INFO 03-29 14:42:04 model_runner.py:1562] Graph capturing finished in 63 secs, took 3.04 GiB
[1;36m(VllmWorkerProcess pid=2893843)[0;0m INFO 03-29 14:42:04 model_runner.py:1562] Graph capturing finished in 63 secs, took 3.03 GiB
INFO 03-29 14:42:04 model_runner.py:1562] Graph capturing finished in 62 secs, took 3.04 GiB
[1;36m(VllmWorkerProcess pid=2893846)[0;0m INFO 03-29 14:42:04 model_runner.py:1562] Graph capturing finished in 62 secs, took 3.03 GiB
[1;36m(VllmWorkerProcess pid=2893847)[0;0m INFO 03-29 14:42:04 model_runner.py:1562] Graph capturing finished in 62 secs, took 3.03 GiB
INFO 03-29 14:42:04 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 191.98 seconds
./datas/databricks-dolly-15k/databricks-dolly-15k.json.cgtag_datas.json not exsits
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893846)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 4 has a total capacity of 31.74 GiB of which 447.12 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893845)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 3 has a total capacity of 31.74 GiB of which 441.12 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893844)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 2 has a total capacity of 31.74 GiB of which 393.12 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893848)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 6 has a total capacity of 31.74 GiB of which 399.12 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893849)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 7 has a total capacity of 31.74 GiB of which 447.12 MiB is free. Including non-PyTorch memory, this process has 31.29 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893843)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 395.12 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1153, in forward
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     output = tensor_model_parallel_all_reduce(output_parallel)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return get_tp_group().all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 307, in all_reduce
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return torch.ops.vllm.all_reduce(input_,
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/_ops.py", line 1116, in __call__
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self._op(*args, **(kwargs or {}))
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 114, in all_reduce
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return group._all_reduce_out_place(tensor)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py", line 313, in _all_reduce_out_place
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     return self.device_communicator.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 63, in all_reduce
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out = pynccl_comm.all_reduce(input_)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py", line 122, in all_reduce
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242]     out_tensor = torch.empty_like(in_tensor)
[1;36m(VllmWorkerProcess pid=2893847)[0;0m ERROR 03-29 14:42:26 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 5 has a total capacity of 31.74 GiB of which 399.12 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.34 GiB is allocated by PyTorch, with 48.00 MiB allocated in private pools (e.g., CUDA Graphs), and 508.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 03-29 14:42:28 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 2893843 died, exit code: -15
INFO 03-29 14:42:28 multiproc_worker_utils.py:128] Killing local vLLM worker processes
tag_evol_tag5_7b_yxprompt
INFO 03-29 14:42:47 __init__.py:207] Automatically detected platform cuda.
WARNING 03-29 14:42:47 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-29 14:42:55 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
INFO 03-29 14:42:55 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 14:42:55 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 14:42:56 multiproc_worker_utils.py:300] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 14:42:56 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:42:56 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:42:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:42:58 cuda.py:226] Using XFormers backend.
INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:43:00 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:43:00 pynccl.py:69] vLLM is using nccl==2.21.5
WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894865)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894869)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894867)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894866)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894870)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894864)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2894868)[0;0m WARNING 03-29 14:43:01 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 03-29 14:43:01 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_bb1b36cd'), local_subscribe_port=59643, remote_subscribe_port=None)
INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894867)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894868)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894866)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894870)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894869)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894865)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=2894864)[0;0m INFO 03-29 14:43:01 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
