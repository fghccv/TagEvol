instruct generation
tag_evol_tag3_7b_yxprompt
INFO 03-29 14:49:29 __init__.py:207] Automatically detected platform cuda.
WARNING 03-29 14:49:29 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-29 14:49:36 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.
INFO 03-29 14:49:36 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 14:49:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 14:49:37 multiproc_worker_utils.py:300] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 14:49:37 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:37 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:37 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:37 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:38 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-29 14:49:38 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:38 cuda.py:226] Using XFormers backend.
INFO 03-29 14:49:38 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:38 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:38 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:38 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:38 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:39 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:39 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:39 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:39 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-29 14:49:39 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:39 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 14:49:39 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:39 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2895848)[0;0m WARNING 03-29 14:49:40 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2895849)[0;0m WARNING 03-29 14:49:40 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2895847)[0;0m WARNING 03-29 14:49:40 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 03-29 14:49:40 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 03-29 14:49:40 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_74dcf4c7'), local_subscribe_port=47857, remote_subscribe_port=None)
INFO 03-29 14:49:40 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:49:40 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:49:40 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:49:40 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
INFO 03-29 14:50:00 model_runner.py:1115] Loading model weights took 3.5547 GB
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:50:00 model_runner.py:1115] Loading model weights took 3.5547 GB
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:50:00 model_runner.py:1115] Loading model weights took 3.5547 GB
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:50:00 model_runner.py:1115] Loading model weights took 3.5547 GB
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:50:20 worker.py:267] Memory profiling takes 19.43 seconds
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:50:20 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:50:20 worker.py:267] model weights take 3.55GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 22.80GiB.
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:50:20 worker.py:267] Memory profiling takes 19.42 seconds
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:50:20 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:50:20 worker.py:267] model weights take 3.55GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 22.84GiB.
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:50:20 worker.py:267] Memory profiling takes 19.46 seconds
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:50:20 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:50:20 worker.py:267] model weights take 3.55GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 22.80GiB.
INFO 03-29 14:50:20 worker.py:267] Memory profiling takes 19.52 seconds
INFO 03-29 14:50:20 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB
INFO 03-29 14:50:20 worker.py:267] model weights take 3.55GiB; non_torch_memory takes 0.29GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 22.75GiB.
INFO 03-29 14:50:20 executor_base.py:111] # cuda blocks: 106505, # CPU blocks: 18724
INFO 03-29 14:50:20 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 52.00x
[1;36m(VllmWorkerProcess pid=2895849)[0;0m INFO 03-29 14:50:26 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-29 14:50:26 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2895847)[0;0m INFO 03-29 14:50:26 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2895848)[0;0m INFO 03-29 14:50:26 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
tag_evol_tag5_7b_yxprompt
INFO 03-29 14:50:51 __init__.py:207] Automatically detected platform cuda.
WARNING 03-29 14:50:51 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-29 14:50:58 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.
INFO 03-29 14:50:58 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 14:50:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 14:50:59 multiproc_worker_utils.py:300] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 14:50:59 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:50:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:50:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:50:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 03-29 14:51:01 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-29 14:51:01 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:51:01 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:51:01 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:51:01 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:51:01 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:51:01 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:51:01 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:51:02 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 14:51:02 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:51:02 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-29 14:51:02 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:51:02 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:51:02 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:51:02 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:51:02 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896198)[0;0m WARNING 03-29 14:51:03 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2896197)[0;0m WARNING 03-29 14:51:03 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2896196)[0;0m WARNING 03-29 14:51:03 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 03-29 14:51:03 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 03-29 14:51:03 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_be682e04'), local_subscribe_port=59399, remote_subscribe_port=None)
INFO 03-29 14:51:03 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896197)[0;0m INFO 03-29 14:51:03 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896198)[0;0m INFO 03-29 14:51:03 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896196)[0;0m INFO 03-29 14:51:03 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 220, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.mlp = Qwen2MLP(
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 75, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 441, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     super().__init__(input_size=input_size,
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 314, in __init__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896196)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 43.12 MiB is free. Process 2895847 has 28.00 GiB memory in use. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 2.99 GiB is allocated by PyTorch, and 237.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 208, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.self_attn = Qwen2Attention(
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 145, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.o_proj = RowParallelLinear(
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1062, in __init__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896198)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 31.74 GiB of which 3.12 MiB is free. Process 2895849 has 27.96 GiB memory in use. Including non-PyTorch memory, this process has 3.76 GiB memory in use. Of the allocated memory 3.09 GiB is allocated by PyTorch, and 232.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 220, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.mlp = Qwen2MLP(
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 75, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 441, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     super().__init__(input_size=input_size,
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 314, in __init__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896197)[0;0m ERROR 03-29 14:51:04 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 2 has a total capacity of 31.74 GiB of which 41.12 MiB is free. Process 2895848 has 28.00 GiB memory in use. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 2.99 GiB is allocated by PyTorch, and 237.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
INFO 03-29 14:51:14 model_runner.py:1115] Loading model weights took 3.5547 GB
ERROR 03-29 14:51:14 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 2896198 died, exit code: -15
INFO 03-29 14:51:14 multiproc_worker_utils.py:128] Killing local vLLM worker processes
tag_evol_tag7_7b_yxprompt
INFO 03-29 14:51:21 __init__.py:207] Automatically detected platform cuda.
WARNING 03-29 14:51:21 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-29 14:51:28 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
INFO 03-29 14:51:28 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 14:51:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 14:51:28 multiproc_worker_utils.py:300] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 14:51:29 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:30 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:30 cuda.py:226] Using XFormers backend.
INFO 03-29 14:51:30 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-29 14:51:30 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:31 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:31 cuda.py:226] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:31 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:31 cuda.py:226] Using XFormers backend.
INFO 03-29 14:51:32 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:32 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 14:51:32 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:32 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:32 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:32 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:32 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:32 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2896404)[0;0m WARNING 03-29 14:51:33 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2896402)[0;0m WARNING 03-29 14:51:33 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 03-29 14:51:33 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2896403)[0;0m WARNING 03-29 14:51:33 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 03-29 14:51:33 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f95664f4'), local_subscribe_port=50667, remote_subscribe_port=None)
INFO 03-29 14:51:33 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896403)[0;0m INFO 03-29 14:51:33 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896404)[0;0m INFO 03-29 14:51:33 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896402)[0;0m INFO 03-29 14:51:33 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 220, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.mlp = Qwen2MLP(
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 75, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 441, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     super().__init__(input_size=input_size,
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 314, in __init__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896403)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 2 has a total capacity of 31.74 GiB of which 41.12 MiB is free. Process 2895848 has 28.00 GiB memory in use. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 2.99 GiB is allocated by PyTorch, and 237.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 208, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.self_attn = Qwen2Attention(
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 145, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.o_proj = RowParallelLinear(
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1062, in __init__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896404)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 31.74 GiB of which 3.12 MiB is free. Process 2895849 has 27.96 GiB memory in use. Including non-PyTorch memory, this process has 3.76 GiB memory in use. Of the allocated memory 3.09 GiB is allocated by PyTorch, and 232.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 453, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 307, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 557, in make_layers
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 558, in <listcomp>
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 309, in <lambda>
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     lambda prefix: Qwen2DecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 220, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.mlp = Qwen2MLP(
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 75, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 441, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     super().__init__(input_size=input_size,
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 314, in __init__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=2896402)[0;0m ERROR 03-29 14:51:34 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 43.12 MiB is free. Process 2895847 has 28.00 GiB memory in use. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 2.99 GiB is allocated by PyTorch, and 237.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
INFO 03-29 14:51:40 model_runner.py:1115] Loading model weights took 3.5547 GB
ERROR 03-29 14:51:40 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 2896402 died, exit code: -15
INFO 03-29 14:51:40 multiproc_worker_utils.py:128] Killing local vLLM worker processes
