instruct generation
tag_evol_tag3_7b_yxprompt
INFO 03-29 05:55:47 __init__.py:207] Automatically detected platform cuda.
INFO 03-29 05:55:53 config.py:549] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 03-29 05:55:53 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 05:55:53 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 05:55:53 multiproc_worker_utils.py:300] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 05:55:54 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 03-29 05:55:55 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:55 cuda.py:229] Using Flash Attention backend.
INFO 03-29 05:55:56 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:56 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 05:55:56 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:56 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-29 05:55:57 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sqshou/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:57 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sqshou/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-29 05:55:57 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_3bd60e2e'), local_subscribe_port=51461, remote_subscribe_port=None)
INFO 03-29 05:55:57 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:55:57 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 05:59:36 model_runner.py:1115] Loading model weights took 67.8002 GB
INFO 03-29 05:59:36 model_runner.py:1115] Loading model weights took 67.8002 GB
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:06 worker.py:267] Memory profiling takes 30.00 seconds
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:06 worker.py:267] the current vLLM instance can use total_gpu_memory (79.20GiB) x gpu_memory_utilization (1.00) = 79.20GiB
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:06 worker.py:267] model weights take 67.80GiB; non_torch_memory takes 0.45GiB; PyTorch activation peak memory takes 5.24GiB; the rest of the memory reserved for KV Cache is 5.71GiB.
INFO 03-29 06:00:07 worker.py:267] Memory profiling takes 30.33 seconds
INFO 03-29 06:00:07 worker.py:267] the current vLLM instance can use total_gpu_memory (79.20GiB) x gpu_memory_utilization (1.00) = 79.20GiB
INFO 03-29 06:00:07 worker.py:267] model weights take 67.80GiB; non_torch_memory takes 0.46GiB; PyTorch activation peak memory takes 5.24GiB; the rest of the memory reserved for KV Cache is 5.70GiB.
INFO 03-29 06:00:07 executor_base.py:111] # cuda blocks: 2335, # CPU blocks: 1638
INFO 03-29 06:00:07 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 1.14x
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-29 06:00:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-29 06:00:32 custom_all_reduce.py:226] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:32 custom_all_reduce.py:226] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=3788492)[0;0m INFO 03-29 06:00:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.50 GiB
INFO 03-29 06:00:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.50 GiB
INFO 03-29 06:00:33 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 56.33 seconds
./datas/databricks-dolly-15k/databricks-dolly-15k.json.cgtag_datas.json not exsits
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/utils.py", line 2196, in run_method
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 91, in start_worker_execution_loop
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1724, in execute_model
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 486, in forward
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     hidden_states = self.model(input_ids, positions, kv_caches,
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 172, in __call__
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 348, in forward
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     hidden_states, residual = layer(
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 257, in forward
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     hidden_states = self.mlp(hidden_states)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 97, in forward
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     x, _ = self.down_proj(x)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1149, in forward
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     output_parallel = self.quant_method.apply(self,
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]   File "/home/sqshou/anaconda3/envs/thm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 142, in apply
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242]     return F.linear(x, layer.weight, bias)
[1;36m(VllmWorkerProcess pid=3788492)[0;0m ERROR 03-29 06:00:49 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 1 has a total capacity of 79.20 GiB of which 490.62 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 77.29 GiB is allocated by PyTorch, with 66.00 MiB allocated in private pools (e.g., CUDA Graphs), and 93.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
tag_evol_tag5_7b_yxprompt
INFO 03-29 06:00:58 __init__.py:207] Automatically detected platform cuda.
INFO 03-29 06:01:04 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.
INFO 03-29 06:01:04 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-29 06:01:04 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/share/models/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='/home/share/models/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/share/models/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 03-29 06:01:04 multiproc_worker_utils.py:300] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-29 06:01:04 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:04 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 03-29 06:01:06 cuda.py:229] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:06 cuda.py:229] Using Flash Attention backend.
INFO 03-29 06:01:06 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:06 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-29 06:01:06 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:06 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:07 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sqshou/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-29 06:01:07 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sqshou/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-29 06:01:07 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c4d3744c'), local_subscribe_port=33099, remote_subscribe_port=None)
INFO 03-29 06:01:07 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
[1;36m(VllmWorkerProcess pid=3788968)[0;0m INFO 03-29 06:01:07 model_runner.py:1110] Starting to load model /home/share/models/Qwen2.5-72B-Instruct...
